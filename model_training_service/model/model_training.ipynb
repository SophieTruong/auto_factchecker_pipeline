{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b729eb-22d6-478f-87f4-e9fb698652b7",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_training.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "sys.path.append(os.path.join(\n",
    "                os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "               )\n",
    "\n",
    "sys.path.append(os.path.join(\n",
    "                os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))),\n",
    "                'model_training_service')\n",
    "               )\n",
    "sys.path.append(os.path.join(\n",
    "                os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))),\n",
    "                'model_training_service',\n",
    "                'data')\n",
    "               )\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "from data.data_processing import get_text_label, get_data_splits, shape_data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments, sample_dataset\n",
    "from sentence_transformers import InputExample, SentenceTransformer, losses\n",
    "\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, accuracy_score, confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "\n",
    "# Model tracking\n",
    "import wandb\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.pyfunc import PythonModel\n",
    "\n",
    "\n",
    "# Model parameter tuning\n",
    "import optuna\n",
    "\n",
    "class SetFitCustomModel(PythonModel):\n",
    "    def load_context(self, context):\n",
    "        self.model = SetFitModel.from_pretrained(context.artifacts['snapshot'])\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        predicts = self.model.predict(model_input)\n",
    "        return predicts\n",
    "\n",
    "def get_or_create_experiment(experiment_name):\n",
    "    \"\"\"\n",
    "    Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.\n",
    "\n",
    "    This function checks if an experiment with the given name exists within MLflow.\n",
    "    If it does, the function returns its ID. If not, it creates a new experiment\n",
    "    with the provided name and returns its ID.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment_name (str): Name of the MLflow experiment.\n",
    "\n",
    "    Returns:\n",
    "    - str: ID of the existing or newly created MLflow experiment.\n",
    "    \"\"\"\n",
    "\n",
    "    if experiment := mlflow.get_experiment_by_name(experiment_name):\n",
    "        return experiment.experiment_id\n",
    "    else:\n",
    "        return mlflow.create_experiment(experiment_name)\n",
    "\n",
    "# define a logging callback that will report on only new challenger parameter configurations if a\n",
    "# trial has usurped the state of 'best conditions'\n",
    "def champion_callback(study, frozen_trial):\n",
    "    \"\"\"\n",
    "    Logging callback that will report when a new trial iteration improves upon existing\n",
    "    best trial values.\n",
    "\n",
    "    Note: This callback is not intended for use in distributed computing systems such as Spark\n",
    "    or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n",
    "    workers or agents.\n",
    "    The race conditions with file system state management for distributed trials will render\n",
    "    inconsistent values with this callback.\n",
    "    \"\"\"\n",
    "\n",
    "    winner = study.user_attrs.get(\"winner\", None)\n",
    "\n",
    "    if study.best_value and winner != study.best_value:\n",
    "        study.set_user_attr(\"winner\", study.best_value)\n",
    "        if winner:\n",
    "            improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n",
    "            print(\n",
    "                f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n",
    "                f\"{improvement_percent: .4f}% improvement\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Load data\n",
    "    data_path = os.path.join(\n",
    "                os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))),\n",
    "                'model_training_service', \n",
    "                os.environ.get(\"DATA_PATH\")\n",
    "    )\n",
    "    \n",
    "    texts, labels = get_text_label(data_path)\n",
    "    \n",
    "    # Create data splits\n",
    "    train_df, test_df1, dev_df, test_scores= get_data_splits(1, texts, labels)\n",
    "            \n",
    "    train_ds, test_ds, dev_ds, test_df= shape_data(train_df, test_df1, dev_df, False)\n",
    "\n",
    "    # Set model\n",
    "    model, pretrained_model_name = set_model()\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        \n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        uuid_str = str(uuid.uuid4())\n",
    "        \n",
    "        print(f\"Run ID: {run_id}, UUID: {uuid_str}\")\n",
    "        \n",
    "        # Define hyperparameters\n",
    "        params = {\n",
    "            \"num_epochs\": trial.suggest_int(\"num_epochs\", 2, 4, log=True),\n",
    "            \"batch_size\": trial.suggest_categorical(\"batch_size\", [16,32]),\n",
    "            \"num_iterations\": trial.suggest_int(\"num_iterations\", 6, 8, log=True),\n",
    "        }\n",
    "        \n",
    "        print(f\"Hyperparameters during trial: {params}\") \n",
    "        \n",
    "        # TrainingArguments\n",
    "        training_args = TrainingArguments(\n",
    "            num_epochs=params[\"num_epochs\"],\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            num_iterations=params[\"num_iterations\"],\n",
    "            loss=losses.CosineSimilarityLoss,\n",
    "            report_to=\"mlflow\",\n",
    "        )\n",
    "        \n",
    "        training_args.eval_strategy = training_args.eval_strategy\n",
    "\n",
    "        # Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds,      \n",
    "            eval_dataset=dev_ds,         \n",
    "            metric= f1_score_weighted\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        trainer.train()\n",
    "        \n",
    "        # Evaluate model\n",
    "        metric = trainer.evaluate()\n",
    "        print(f\"Hyperparameters: {params}, f1_score_weighted: {metric['metric']}\")\n",
    "\n",
    "        # Log to MLflow\n",
    "        \n",
    "        mlflow.log_params({f\"{uuid_str}_{k}\":v for k,v in params.items()})\n",
    "        \n",
    "        mlflow.log_metric(\"f1_score_weighted\", metric['metric'])\n",
    "        \n",
    "    return metric['metric']\n",
    "\n",
    "\n",
    "def set_model(multi_label=False):\n",
    "    # Load a SetFit model from Hub \n",
    "    pretrained_model_name = 'FacebookAI/xlm-roberta-base' #'TurkuNLP/bert-base-finnish-cased-v1'\n",
    "    if multi_label:\n",
    "        model = SetFitModel.from_pretrained(pretrained_model_name, multi_target_strategy=\"multi-output\")\n",
    "    else:\n",
    "        model = SetFitModel.from_pretrained(pretrained_model_name)\n",
    "    return model, pretrained_model_name\n",
    "\n",
    "def config_wandb(multi_label, pretrained_model_name, config=None):\n",
    "    if config is None:\n",
    "        config = {'learning_rate': 3.0191843531454982e-05, 'num_epochs': 4, 'batch_size': 16, 'seed': 34, 'num_iterations': 6}\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"Claim-detection\",\n",
    "        notes=\"initial\",\n",
    "        tags=[\"Henna\", \"binary\", \"single label\" if multi_label else 'binary', 'first_test', 'DIME_data_HennaPipsaMinttu', 'ei-tark_henkkoht_muu==0', pretrained_model_name],\n",
    "        config=config,\n",
    "    )\n",
    "    return wandb, config \n",
    "    \n",
    "def f1_score_weighted(y_true, y_pred):\n",
    "    return f1_score(y_true.numpy(), np.array(y_pred), average='weighted')\n",
    "\n",
    "def train_model(model, train_ds, dev_ds, config):\n",
    "    print(f\"Training model with config: {config}\")\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_epochs=config[\"num_epochs\"],\n",
    "        eval_strategy=\"epoch\", #\"epoch\",\n",
    "        save_strategy=\"epoch\", #\"epoch\",\n",
    "        load_best_model_at_end=False,\n",
    "        num_iterations=config[\"num_iterations\"],\n",
    "        loss=losses.CosineSimilarityLoss,\n",
    "        report_to=\"mlflow\",\n",
    "    )\n",
    "    args.eval_strategy = args.evaluation_strategy\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_ds,\n",
    "        args=args,\n",
    "        eval_dataset=dev_ds,\n",
    "        metric= f1_score_weighted\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    metric = trainer.evaluate()\n",
    "    print(metric)\n",
    "    return model, metric, trainer\n",
    "\n",
    "def get_predictions(model, test_df, multi_label):    \n",
    "    # model = new_model\n",
    "    # Run inference\n",
    "    preds = model.predict(test_df['text'], as_numpy=True)\n",
    "    probs = model.predict_proba(test_df['text'], as_numpy=True)\n",
    "    probsmax = np.max(probs, axis=1)\n",
    "\n",
    "    if not multi_label:\n",
    "        # preds_labels = [classes[i] for i in preds]\n",
    "        y_true = test_df['label']\n",
    "        y_pred = preds\n",
    "    else:\n",
    "        y_true = test_df.drop(columns=['text']).values\n",
    "        y_pred = preds\n",
    "    return preds, probs, y_true, y_pred #probsmax\n",
    "\n",
    "\n",
    "def print_confusion_matrix(confusion_matrix_data, axes, class_label, class_names, fontsize=14):\n",
    "\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix_data, index=class_names, columns=class_names,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, ax=axes)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    axes.set_ylabel('True label')\n",
    "    axes.set_xlabel('Predicted label')\n",
    "    axes.set_title(\"Confusion Matrix for the class - \" + class_label)\n",
    "\n",
    "def get_confusion_matrix(multi_label, wandb, y_true, y_pred, classes):\n",
    "    if not multi_label:\n",
    "        wandb.log({\"confusion matrix\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                                y_true=y_true, preds=y_pred,\n",
    "                                class_names=classes)})\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        df_cm = pd.DataFrame(cm, index = classes, columns = classes)\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True)\n",
    "    else:\n",
    "        cm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        fig, ax = plt.subplots(5, 3, figsize=(12, 12))\n",
    "\n",
    "        for axes, cfs_matrix, label in zip(ax.flatten(), cm, classes):\n",
    "            print_confusion_matrix(cfs_matrix, axes, label, [\"N\", \"Y\"])\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.show()   \n",
    "        wandb.log({\"confusion matrix 2\": wandb.Image(plt)})\n",
    "        \n",
    "def get_classification_report(y_true, y_pred):\n",
    "    classes0 = ['True', 'False']\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(y_true, y_pred, target_names=classes0, zero_division=0))\n",
    "\n",
    "def get_evaluation_metrics(y_true, y_pred, probs, preds, wandb):\n",
    "    micro_f1_score = f1_score(y_true, y_pred, average='micro')\n",
    "    wandb.log({\"f1-score micro\": micro_f1_score})\n",
    "    print('f1-score micro', f1_score(y_true, y_pred, average='micro'))\n",
    "\n",
    "    macro_f1_score = f1_score(y_true, y_pred, average='macro')\n",
    "    wandb.log({\"f1-score macro\": macro_f1_score})\n",
    "    print('f1-score macro', macro_f1_score)\n",
    "\n",
    "    weighted_f1_score = f1_score(y_true, y_pred, average='weighted')\n",
    "    wandb.log({\"f1-score weighted\": weighted_f1_score})\n",
    "    print('f1-score weighted', weighted_f1_score)\n",
    "\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    wandb.log({\"balanced accuracy\": balanced_accuracy})\n",
    "    print('balanced accuracy', balanced_accuracy)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    wandb.log({\"accuracy\": accuracy})\n",
    "    print('accuracy ', accuracy)\n",
    "    \n",
    "    return balanced_accuracy, accuracy, macro_f1_score, micro_f1_score, weighted_f1_score\n",
    "    \n",
    "def write_metrics_to_file(state_r, balanced_accuracy, accuracy, macro_f1_score, micro_f1_score, weighted_f1_score,pretrained_model_name):\n",
    "    lines= [' ,', ' ,',pretrained_model_name, ' ,',str(state_r),' ,', 'DIME_data_HennaMinttuPipsa_all_labels',' ,', 'balanced_accuracy:',' ,', str(balanced_accuracy),' ,', 'accuracy:',' ,', str(accuracy),' ,',\n",
    "       'macro-f1:',' ,', str(macro_f1_score),' ,', 'micro_f1_score:',' ,', str(micro_f1_score),' ,', 'weighted f1:', ' ,',str(weighted_f1_score),' ,', '\\n']\n",
    "\n",
    "    with open('setfit_claim_detection_DIME_data_HennaMinttuPipsa_all_labels_henkkoht-eitarkist-muu==0.txt', 'a') as f:\n",
    "        f.writelines(lines)\n",
    "        \n",
    "def main():\n",
    "    # setting device on GPU if available, else CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "    print()\n",
    "    \n",
    "    #Additional Info when using cuda\n",
    "    if device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "\n",
    "    # Load data\n",
    "    data_path = os.path.join(\n",
    "                os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))),\n",
    "                'model_training_service', \n",
    "                os.environ.get(\"DATA_PATH\")\n",
    "    )\n",
    "    multi_label = False\n",
    "\n",
    "    # Inspect the data reading, spliting code\n",
    "    random_states=[7,8,9,10]#[1,2,3,4sla,5,6,]\n",
    "\n",
    "    texts, labels = get_text_label(data_path)\n",
    "    \n",
    "    # override Optuna's default logging to ERROR only\n",
    "    optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "    \n",
    "    # Set up MLFlow tracking\n",
    "    experiment_name = f\"/Users/{os.getenv('MLFLOW_TRACKING_USERNAME')}/claim-detection-setfit-Facebook-XML\"\n",
    "    experiment_id = get_or_create_experiment(experiment_name)\n",
    "    print(f\"Experiment ID: {experiment_id}\")\n",
    "    print(f\"Experiment Name: {experiment_name}\")\n",
    "    \n",
    "    mlflow.set_experiment(\n",
    "        experiment_id = experiment_id\n",
    "    )\n",
    "    run_name = \"setfit-Facebook-XML-Henna-training\" \n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):\n",
    "        # Initialize the Optuna study\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "        # Execute the hyperparameter optimization trials.\n",
    "        # Note the addition of the `champion_callback` inclusion to control our logging\n",
    "        study.optimize(objective, n_trials=10, callbacks=[champion_callback])\n",
    "        \n",
    "        optuna_best_params = study.best_params\n",
    "        optuna_best_value = study.best_value\n",
    "        print(f\"Optuna best params: {optuna_best_params}, Optuna best value: {optuna_best_value}\")\n",
    "        \n",
    "        # Log the best hyperparameters\n",
    "        mlflow.log_params(optuna_best_params)\n",
    "        mlflow.log_metric(\"best_f1\", optuna_best_value)\n",
    "        \n",
    "        # Log tags\n",
    "        mlflow.set_tags(\n",
    "            tags={\n",
    "                \"project\": \"DIME Claim Detection model\",\n",
    "                \"optimizer_engine\": \"optuna\",\n",
    "                \"model_family\": \"setfit-Facebook-XML\",\n",
    "                \"feature_set_version\": 1,\n",
    "                \"dataset\": \"DIME_data_HennaPipsaMinttu\",\n",
    "                \"label\": \"ei-tark_henkkoht_muu==0\",\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        for state_x in random_states:\n",
    "            with mlflow.start_run(nested=True):\n",
    "                train_df, test_df1, dev_df, test_scores= get_data_splits(state_x, texts, labels)\n",
    "            \n",
    "                train_ds, test_ds, dev_ds, test_df= shape_data(train_df, test_df1, dev_df, multi_label)\n",
    "            \n",
    "                model, pretrained_model_name = set_model()\n",
    "            \n",
    "                wandb, config= config_wandb(multi_label, pretrained_model_name, optuna_best_params) \n",
    "            \n",
    "                model, metric, trainer= train_model(model, train_ds, dev_ds, config) \n",
    "                \n",
    "                preds, probs, y_true, y_pred= get_predictions(model, test_df, multi_label)\n",
    "            \n",
    "                get_classification_report(y_true, y_pred)\n",
    "            \n",
    "                balanced_accuracy, accuracy, macro_f1_score, micro_f1_score, weighted_f1_score= get_evaluation_metrics(y_true, y_pred,probs, preds, wandb)\n",
    "            \n",
    "                write_metrics_to_file(state_x, balanced_accuracy, accuracy, macro_f1_score, micro_f1_score, weighted_f1_score,pretrained_model_name)\n",
    "                            \n",
    "                # First, save the finetuned model locally\n",
    "                model.save_pretrained('snapshot')\n",
    "\n",
    "                # Log the hyperparameters\n",
    "                config[\"train_test_split_random_state\"] = state_x\n",
    "                mlflow.log_params({\n",
    "                    \"train_test_split_random_state\":state_x,\n",
    "                    \"device\": torch.cuda.get_device_name(0)\n",
    "                })\n",
    "\n",
    "                # Log the loss metric\n",
    "                mlflow.log_metric(\"f1-score micro\", micro_f1_score)\n",
    "                mlflow.log_metric(\"f1-score macro\", macro_f1_score)\n",
    "                mlflow.log_metric(\"f1-score weighted\", weighted_f1_score)\n",
    "                mlflow.log_metric(\"balanced accuracy\", balanced_accuracy)\n",
    "                mlflow.log_metric(\"accuracy\", accuracy)\n",
    "                \n",
    "                # Log the model\n",
    "                artifact_path = \"setfit_model\"\n",
    "                model_info = mlflow.pyfunc.log_model(\n",
    "                    artifact_path=artifact_path,\n",
    "                    artifacts={'snapshot': 'snapshot'},\n",
    "                    python_model=SetFitCustomModel(),\n",
    "                    conda_env='conda-env.yml',\n",
    "                    registered_model_name=\"claim-detection-setfit-TurkuNLP\",\n",
    "                )\n",
    "                # Get the logged model uri so that we can load it from the artifact store\n",
    "                model_uri = mlflow.get_artifact_uri(artifact_path)\n",
    "                print(model_uri)\n",
    "                print('MLFLOW model_uri: %s', model_uri)\n",
    "                print('SUCCEED')\n",
    "                with open('mlflow_model_uri.json', 'w') as f:\n",
    "                    json.dump({\n",
    "                        \"model_uri\": model_uri, \n",
    "                        \"experiment_id\": experiment_id, \n",
    "                        \"artifact_path\": artifact_path,\n",
    "                        \"pretrained_model_name\": pretrained_model_name,\n",
    "                        \"config\": config\n",
    "                    }, f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46a269b6-aae1-44f6-9d10-952414b0e8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/work/truongl3/.conda_envs/claim-model-training-env/lib/python312.zip', '/scratch/work/truongl3/.conda_envs/claim-model-training-env/lib/python3.12', '/scratch/work/truongl3/.conda_envs/claim-model-training-env/lib/python3.12/lib-dynload', '', '/home/truongl3/.local/lib/python3.12/site-packages', '/scratch/work/truongl3/.conda_envs/claim-model-training-env/lib/python3.12/site-packages', '/scratch/work/truongl3/DIME/auto_factchecker_pipeline/model_training_service', '/scratch/work/truongl3/DIME/auto_factchecker_pipeline/model_training_service/Henna_claim_detection_model', '/scratch/work/truongl3/.conda_envs/claim-model-training-env/lib/python3.12/site-packages/setuptools/_vendor', '/tmp/tmp22q1itnj', '/scratch/work/truongl3/DIME/auto_factchecker_pipeline/model_training_service', '/scratch/work/truongl3/DIME/auto_factchecker_pipeline/model_training_service/Henna_claim_detection_model']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(\n",
    "                os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "               )\n",
    "\n",
    "sys.path.append(os.path.join(\n",
    "                os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))),\n",
    "                'Henna_claim_detection_model')\n",
    "               )\n",
    "print(sys.path)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "from data.data_processing import get_text_label, get_data_splits, shape_data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments, sample_dataset\n",
    "from sentence_transformers import InputExample, SentenceTransformer, losses\n",
    "\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, accuracy_score, confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "import torch\n",
    "\n",
    "# Model tracking\n",
    "import wandb\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.pyfunc import PythonModel\n",
    "\n",
    "\n",
    "# Model parameter tuning\n",
    "import optuna\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58798d76-bbc5-4541-9031-b2214a883fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///scratch/work/truongl3/DIME/auto_factchecker_pipeline/model_training_service/mlruns/518773968925100294/87130472c45a47e39c63e418d91f967d/artifacts/setfit_model'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7429943-69c3-45b2-80db-7d8eaa343f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///scratch/work/truongl3/DIME/auto_factchecker_pipeline/model_training_service/mlruns/518773968925100294/87130472c45a47e39c63e418d91f967d/artifacts/setfit_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nReuters: Trumpin Ukrainan-rauhanlähettilään vierailu Kiovaan lykkääntyy\\n\\nDonald Trumpin hallinnon Ukrainan rauhanlähettiläs Keith Kellogg on lykännyt Kiovan-vierailuaan siihen asti, kunnes Trump on astunut presidentin virkaan tammikuun 20',\n",
       " 'päivä, uutistoimisto Reuters kertoo asiasta tietäviin lähteisiin viitaten.\\n\\nReutersille puhuneiden lähteiden mukaan uutta päivämäärää ei ole vielä päätetty, eikä ole selvää, miksi vierailua lykätään',\n",
       " 'Ukrainan presidentti Volodymyr Zelenskyi sanoi joulukuussa toimittajille Brysselissä, että Kellogg vierailisi Ukrainassa ennen Trumpin virkaanastujaisia.\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../mlflow_model_uri.json\", \"r\") as fn:\n",
    "    model_uri = json.load(fn)['model_uri']\n",
    "print(model_uri)\n",
    "\n",
    "test_input_str = \"\"\"\n",
    "Reuters: Trumpin Ukrainan-rauhanlähettilään vierailu Kiovaan lykkääntyy\n",
    "\n",
    "Donald Trumpin hallinnon Ukrainan rauhanlähettiläs Keith Kellogg on lykännyt Kiovan-vierailuaan siihen asti, kunnes Trump on astunut presidentin virkaan tammikuun 20. päivä, uutistoimisto Reuters kertoo asiasta tietäviin lähteisiin viitaten.\n",
    "\n",
    "Reutersille puhuneiden lähteiden mukaan uutta päivämäärää ei ole vielä päätetty, eikä ole selvää, miksi vierailua lykätään. Ukrainan presidentti Volodymyr Zelenskyi sanoi joulukuussa toimittajille Brysselissä, että Kellogg vierailisi Ukrainassa ennen Trumpin virkaanastujaisia.\n",
    "\"\"\".split(\". \")\n",
    "display(test_input_str)\n",
    "\n",
    "test_input = test_input_str\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "# print(loaded_model.predict([\"A\", \"B\", \"C\"]))  # -> [\"A\", \"B\", \"C\"]\n",
    "loaded_model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "beeeb8a1-b5b9-4bc5-9a12-41399677c997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = SetFitModel.from_pretrained('/scratch/work/truongl3/DIME/Henna_claim_detection_model/snapshot')\n",
    "ml(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ece440e5-87cc-4f63-bd15-982ffefb000d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/work/truongl3/.conda_envs/claim-model-training-env/lib/python312.zip', '/scratch/work/truongl3/.conda_envs/claim-model-training-env/lib/python3.12', '/scratch/work/truongl3/.conda_envs/claim-model-training-env/lib/python3.12/lib-dynload', '', '/home/truongl3/.local/lib/python3.12/site-packages', '/scratch/work/truongl3/.conda_envs/claim-model-training-env/lib/python3.12/site-packages', '/scratch/work/truongl3/DIME/Henna_claim_detection_model', '/scratch/work/truongl3/DIME/Henna_claim_detection_model/Henna_claim_detection_model', '/scratch/work/truongl3/.conda_envs/claim-model-training-env/lib/python3.12/site-packages/setuptools/_vendor', '/tmp/tmpg1m52e37', '/scratch/work/truongl3/DIME/Henna_claim_detection_model', '/scratch/work/truongl3/DIME/Henna_claim_detection_model/Henna_claim_detection_model', '/scratch/work/truongl3/DIME/Henna_claim_detection_model', '/scratch/work/truongl3/DIME/Henna_claim_detection_model/data', '/scratch/work/truongl3/DIME/Henna_claim_detection_model', '/scratch/work/truongl3/DIME/Henna_claim_detection_model/data']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(\n",
    "                os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "               )\n",
    "\n",
    "sys.path.append(os.path.join(\n",
    "                os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))),\n",
    "                'data')\n",
    "               )\n",
    "print(sys.path)\n",
    "from data.data_processing import get_text_label, get_data_splits, shape_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claim-model-training-env",
   "language": "python",
   "name": "internal_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
